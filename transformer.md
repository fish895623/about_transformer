# 트랜스포머 모델

seq2seq 의 한계를 해결하기 위한 새로운 모델

seq2seq 은 인코더가 입력 시퀀스를 한개의 벡터에 압축해서 표현하여 데이터손실이 일어남

이 과정중 어탠션으로 데이터 손실을 보정하기 시작했고 부가적으로 어텐션 만으로 인코더와 만든 모델이 트랜스포머

#### 모델 구성요소

* 인코더
* 포지셔널 인코딩
* 어텐션
    * 멀티 헤드 어텐션
* 디코더
  * 셀프 어텐션
  * 룩 어헤드 마스크 ( Look Ahead Mask )
* 포지션 와이즈 피드 포워드 신경망 ( Feed-Forward Neural Network - FFNN )

---

## 트랜스포머 모델의 형태

seq2seq 와 형태는 비슷하나 N개의 인코더와 디코더의 갯수를 넣을수 있음

---

## 인코더

### 인코더의 구조

트랜스포머는 N개의 인코더 갯수를 쌓음

* 인코더
    * 포지션 와이즈 FFNN
    * 멀티 헤드 셀프 어텐션
        * ... 셀프어텐션 여러개를 병렬적으로 사용

---

## 포지셔널 인코딩

임베딩 벡터에 각 단어의 위치 정보들을 더하여 모델에 입력으로 사용함

---

## 어텐션

* 인코더의 Self-Attention                : Query = Key = Value
* 디코더의 Masked Decoder Self-Attention : Query = Key = Value
* 디코더의 Encoder-Decoder Attention   : Query ( 디코더 벡터 ) / Key = Value ( 인코더 벡터 )

### 멀티 헤드 어텐션

한번의 어텐션을 시행하는것 보다 어텐션을 여러번 병렬로 사용하는것이 더욱더 효과적이라고 판단

---

## 디코더

총 두개의 층을 가지게 됨

1. 셀프어텐션, 룩 어헤드 마스크
2. 인코더-디코더 어텐션

### 1. 셀프 어텐션과 룩 어헤드 마스크 

#### 셀프 어텐션

#### 룩어헤드 마스크 ( Look Ahead Mask )

### 2. 인코더-디코더 어텐션
